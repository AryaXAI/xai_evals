# xai_evals

**`xai_evals`** is a Python package designed for explainable AI (XAI) and model interpretability. It provides tools for generating and evaluating explanations of machine learning models, with support for popular explanation methods such as **SHAP** and **LIME**. The package aims to simplify the interpretability of machine learning models, enabling practitioners to understand how their models make predictions. It also includes several metrics for evaluating the quality of these explanations, focusing on tabular data.

---

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
  - [SHAP Explainer](#shap-explainer)
  - [LIME Explainer](#lime-explainer)
  - [Metrics Calculation](#metrics-calculation)
- [Extending with More Explanations](#extending-with-more-explanations)
- [Contributing](#contributing)
- [License](#license)

---

## Installation

To install **`xai_evals`**, you can use `pip`. First, clone the repository or download the files to your local environment. Then, install the necessary dependencies:

```bash
git clone https://github.com/yourusername/xai_evals.git
cd xai_evals
pip install -e .
```

Alternatively, if you don't want to clone the repo manually, you can install the package directly from pip (after publishing it [TODO]).

### Dependencies

- `shap`: A library for SHAP values (SHapley Additive exPlanations).
- `lime`: A library for LIME (Local Interpretable Model-Agnostic Explanations).
- `xgboost`: A gradient boosting library.
- `scikit-learn`: For machine learning models and utilities.
- `torch`: For deep learning model support (optional).
- `pandas`: For data handling.
- `numpy`: For numerical computations.

To install all dependencies, run:

```bash
pip install -r requirements.txt
```

---

## Usage

### SHAP Explainer

The `SHAPExplainer` class allows you to compute and visualize **SHAP** values for your trained model. It supports various types of models, including tree-based models (e.g., `RandomForest`, `XGBoost`) and deep learning models (e.g., PyTorch models).

**Example:**

```python
from xai_evals.explainer import SHAPExplainer
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.datasets import load_iris

# Load dataset and train a model
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target
model = RandomForestClassifier()
model.fit(X, y)

# Initialize SHAP explainer
shap_explainer = SHAPExplainer(model=model, features=X.columns, task="classification", X_train=X)

# Explain a specific instance (e.g., the first instance in the test set)
shap_attributions = shap_explainer.explain(X, instance_idx=0)

# Print the feature attributions
print(shap_attributions)
```

### LIME Explainer

The `LIMEExplainer` class allows you to generate **LIME** explanations, which work by perturbing the input data and fitting a locally interpretable model.

**Example:**

```python
from xai_evals.explainer import LIMEExplainer
from sklearn.linear_model import LogisticRegression
import pandas as pd
from sklearn.datasets import load_iris

# Load dataset and train a model
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target
model = LogisticRegression(max_iter=200)
model.fit(X, y)

# Initialize LIME explainer
lime_explainer = LIMEExplainer(model=model, features=X.columns, task="classification", X_train=X)

# Explain a specific instance (e.g., the first instance in the test set)
lime_attributions = lime_explainer.explain(X, instance_idx=0)

# Print the feature attributions
print(lime_attributions)
```

### Metrics Calculation

**`xai_evals`** includes various metrics to evaluate the quality of the explanations generated by SHAP or LIME. These metrics provide insight into different aspects of the explanation, such as robustness, stability, and how well the explanation aligns with the model's behavior. The following metrics are included in the **ExplanationMetrics** class:

1. **Faithfulness (_faithfulness_correlation)**  
   - **Purpose**: Measures how well the explanation model (e.g., SHAP or LIME) reflects the model's predictions.  
   - **How it works**: The explanation model's attributions are compared to the actual change in model prediction when individual features are perturbed.  
   - **Calculation**: Correlation between attribution values and prediction changes. A high correlation means the explanation is faithful to the model.

2. **Infidelity (_infidelity)**  
   - **Purpose**: Measures how much the attributions deviate from the model’s actual predictions.  
   - **How it works**: Compares the predicted impact (based on attributions) with the actual impact on model output when features are perturbed.  
   - **Calculation**: Squared difference between predicted and actual impact to determine infidelity.

3. **Sensitivity (_sensitivity)**  
   - **Purpose**: Evaluates the stability of attributions under small changes in input features.  
   - **How it works**: Random noise is added to input features, and the attribution values are compared to the original ones.  
   - **Calculation**: The Euclidean distance between original and perturbed attribution values is used to compute sensitivity.

4. **Comprehensiveness (_comprehensiveness)**  
   - **Purpose**: Evaluates whether the most important features fully explain the model’s behavior.  
   - **How it works**: The top-k most important features (based on attribution values) are used to mask the input, and the difference in model predictions with and without those features is computed.  
   - **Calculation**: The difference in model predictions between the original and masked inputs.

5. **Sufficiency (_sufficiency)**  
   - **Purpose**: Assesses whether the top-k features alone are sufficient to explain the model's output.  
   - **How it works**: The model is evaluated based on only the top-k features, and the prediction difference is measured.  
   - **Calculation**: The difference between predictions based on the top-k features and the baseline.

6. **Monotonicity (_monotonicity)**  
   - **Purpose**: Evaluates if attribution values have a consistent directional relationship with the model’s predictions.  
   - **How it works**: Checks if the change in attribution values corresponds to a monotonic change in predictions.  
   - **Calculation**: Verifies that the differences between consecutive attributions are either non-negative or non-positive.

7. **AUC for Top-k Features (_auc_tp)**  
   - **Purpose**: Measures how well the top-k features discriminate between classes.  
   - **How it works**: The top-k features based on attribution values are selected, and the AUC (Area Under the ROC Curve) is calculated for classification tasks.  
   - **Calculation**: AUC score is computed using the top-k features’ attribution values and the model’s predicted probabilities.

8. **Complexity (_complexity)**  
   - **Purpose**: Evaluates the sparsity of the explanation.  
   - **How it works**: Counts how many features are involved in the explanation (i.e., have non-zero attributions).  
   - **Calculation**: Sum of the number of non-zero attribution features across instances.

9. **Sparseness (_sparseness)**  
   - **Purpose**: Measures how sparse (minimal) the explanation is.  
   - **How it works**: Counts the number of features with zero or near-zero attribution. A sparse explanation is typically easier to interpret.  
   - **Calculation**: 1 minus the ratio of non-zero attribution features to the total number of features.

**Summary of Metrics:**

| Metric         | Purpose                                      | Calculation Summary                                        |
|----------------|----------------------------------------------|------------------------------------------------------------|
| Faithfulness   | Measures consistency with model behavior.    | Correlation between attribution values and prediction change.|
| Infidelity     | Measures deviation from model predictions.   | Difference between predicted and actual model impact.      |
| Sensitivity    | Measures stability under small changes.      | Euclidean distance between original and perturbed attributions.|
| Comprehensiveness | Measures if top features explain model output. | Difference in predictions with and without top-k features.  |
| Sufficiency    | Measures if top features alone explain output. | Difference in prediction with only top-k features.         |
| Monotonicity   | Measures consistency in attribution direction. | Checks if attributions change monotonically with predictions.|
| AUC (Top-k)    | Measures discriminatory power of top-k features. | AUC based on top-k features and model predictions.          |
| Complexity     | Measures explanation sparsity.               | Number of non-zero attribution features.                   |
| Sparseness     | Measures how focused the explanation is.     | Fraction of non-zero attribution features.                 |

---

## Extending with More Explanations

We plan to expand this library to include more explanation methods, such as:

- **Integrated Gradients**: An attribution method for deep learning models.
- **DeepLIFT**: A method designed for deep learning models that calculates the contribution of each feature to the model's prediction.
- **KernelSHAP**: A kernel-based approximation of SH

AP values that can be applied to any model.

As we add more explanation methods, the `SHAPExplainer` and `LIMEExplainer` classes will be extended to support them, and new classes may be added for other explanation techniques.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

### Future Plans

In the future, we will continue to improve this library by:

- Adding support for more explanation techniques.
- Enhancing the metrics calculation with more advanced techniques.
- Providing better visualization for SHAP and LIME explanations.
- Improving the documentation and usability of the library.

--- 